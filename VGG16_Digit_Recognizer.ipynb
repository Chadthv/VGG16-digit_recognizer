{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chadthv/VGG16-digit_recognizer/blob/main/VGG16_Digit_Recognizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvB1POzUvX9G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import torchvision.models as models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrUnkJ9XvirW"
      },
      "outputs": [],
      "source": [
        "SEED        = 123\n",
        "N_SPLITS    = 5\n",
        "EPOCHS      = 5\n",
        "BATCH_SIZE  = 64\n",
        "LR          = 1e-3\n",
        "WEIGHT_DECAY= 1e-4\n",
        "IMAGE_SHAPE = (28, 28)\n",
        "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def set_seed(seed=SEED):\n",
        "    import random\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "00bUl00wvlmW",
        "outputId": "5b673d85-cb71-4a6d-94c4-10c3c04ce47f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "df_train = pd.read_csv(\"/content/train.csv\")\n",
        "d_test = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "images_np = df_train.drop(\"label\", axis=1).values\n",
        "labels_np = df_train[\"label\"].values\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(images_np[1].reshape(28,28), cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWr4GxWvvo_c"
      },
      "outputs": [],
      "source": [
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Lambda(lambda x: x.expand(3, x.shape[1], x.shape[2])),   # [1,H,W] -> [3,H,W]\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Lambda(lambda x: x.expand(3, x.shape[1], x.shape[2])),   # [1,H,W] -> [3,H,W]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNaGzmzvvyiB"
      },
      "outputs": [],
      "source": [
        "class DigitDataset(Dataset):\n",
        "    def __init__(self, images, labels, image_shape=(28,28), scale=True, add_channel=True, transform=None):\n",
        "        self.images = torch.as_tensor(images, dtype=torch.float32)   # [N, 784]\n",
        "        self.labels = torch.as_tensor(labels, dtype=torch.long)\n",
        "        self.image_shape = image_shape\n",
        "        self.scale = scale\n",
        "        self.add_channel = add_channel\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.images[idx]\n",
        "        y = self.labels[idx]\n",
        "\n",
        "        # skala jika masih 0.255 (hindari double scale)\n",
        "        if self.scale and x.max() > 1:\n",
        "            x = x / 255.0\n",
        "        # reshape flat -> [H,W]\n",
        "        if x.ndim == 1:\n",
        "            x = x.view(*self.image_shape)\n",
        "\n",
        "        # tambah channel -> [1,H,W]\n",
        "        if self.add_channel and x.ndim == 2:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # apply transform (bekerja pada Tensor [C,H,W])\n",
        "        if self.transform is not None:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezqAKD7Ev3kG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class VGG16_net(nn.Module):\n",
        "    def __init__(self, in_channels=3, n_classes=10, dropout: float = 0.5, freeze_backbone=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained VGG16\n",
        "        weights = torchvision.models.VGG16_Weights.IMAGENET1K_V1\n",
        "        net = models.vgg16(weights=weights)\n",
        "\n",
        "        # Nếu muốn đóng băng backbone\n",
        "        if freeze_backbone:\n",
        "            for param in net.features.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Thay lớp cuối bằng Identity để lấy đặc trưng\n",
        "        feat_dim = net.classifier[-1].in_features\n",
        "        net.classifier[-1] = nn.Identity()\n",
        "        self.backbone = net\n",
        "\n",
        "        # Phân loại mới\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(feat_dim, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(4096, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3RcTdZhz7GU"
      },
      "outputs": [],
      "source": [
        "def _warmup_cosine(optimizer, epochs, warmup=3, base_lr=5e-4, min_lr=1e-6):\n",
        "    import math\n",
        "    minf = max(min_lr / max(base_lr, 1e-12), 1e-4)\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < warmup:\n",
        "            return (epoch + 1) / max(1, warmup)\n",
        "        t = (epoch - warmup) / max(1, epochs - warmup)\n",
        "        return minf + (1 - minf) * 0.5 * (1 + math.cos(math.pi * t))\n",
        "    return LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "\n",
        "# Plot traininaning_curves(history, show=True, save_dir=None):\n",
        "    if save_dir is not None:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    epochs = history[\"epoch\"]\n",
        "\n",
        "    return LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "# Loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"train_loss\")\n",
        "    if len(history[\"val_loss\"]) > 0:\n",
        "        plt.plot(epochs, history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training/Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    if save_dir:\n",
        "        plt.savefig(os.path.join(save_dir, \"loss.png\"), bbox_inches=\"tight\")\n",
        "    if show:\n",
        "        plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # Accuracy\n",
        "    if len(history[\"train_acc\"]) > 0:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(epochs, history[\"train_acc\"], label=\"train_acc\")\n",
        "        if len(history[\"val_acc\"]) > 0:\n",
        "            plt.plot(epochs, history[\"val_acc\"], label=\"val_acc\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Accuracy (%)\")\n",
        "        plt.title(\"Training/Validation Accuracy\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        if save_dir:\n",
        "            plt.savefig(os.path.join(save_dir, \"acc.png\"), bbox_inches=\"tight\")\n",
        "        if show:\n",
        "            plt.show()\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO1evyTR2RTf"
      },
      "outputs": [],
      "source": [
        "# Checkpoint\n",
        "def save_checkpoint(model, optimizer, epoch, history, filepath=\"checkpoints/last.pth\"):\n",
        "    \"\"\"Lưu checkpoint của model.\"\"\"\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    checkpoint = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"history\": history\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\" Saved checkpoint at epoch {epoch+1} to {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZlS0pXT2M2I"
      },
      "outputs": [],
      "source": [
        "#Load model\n",
        "def load_checkpoint(model, optimizer, filepath=\"checkpoints/last.pth\", device=\"cpu\"):\n",
        "    \"\"\"Tải checkpoint của model.\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"Checkpoint file {filepath} not found\")\n",
        "\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "    epoch = checkpoint.get(\"epoch\", 0)\n",
        "    history = checkpoint.get(\"history\", {\n",
        "        \"epoch\": [], \"train_loss\": [], \"val_loss\": [],\n",
        "        \"train_acc\": [], \"val_acc\": [], \"lr\": []\n",
        "    })\n",
        "\n",
        "    if \"epoch\" not in checkpoint or \"history\" not in checkpoint:\n",
        "        print(\"Warning: Checkpoint missing 'epoch' or 'history' — resuming from 0\")\n",
        "\n",
        "    print(f\" Resumed from epoch {epoch+1}\")\n",
        "    return model, optimizer, epoch, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzvyFCUG0qDJ"
      },
      "outputs": [],
      "source": [
        "#Train loop\n",
        "def train_model(model, train_loader, val_loader=None,\n",
        "                num_epochs=20, lr=5e-4, weight_decay=1e-4,\n",
        "                label_smoothing=0.05,\n",
        "                warmup_epochs=3, min_lr=1e-6,\n",
        "                patience=10, min_delta=1e-5, max_grad_norm=1.0,\n",
        "                plot=True, save_plots_dir=None, return_history=False,\n",
        "                checkpoint_dir=\"checkpoints\", resume=False, checkpoint_path=\"checkpoints/last.pth\",\n",
        "                csv_path=\"checkpoints/history.csv\"):\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "    crit = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    sch = _warmup_cosine(opt, num_epochs, warmup_epochs, lr, min_lr)\n",
        "\n",
        "    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [],\n",
        "               \"train_acc\": [], \"val_acc\": [], \"lr\": []}\n",
        "\n",
        "    start_epoch = 0\n",
        "    if resume and os.path.exists(checkpoint_path):\n",
        "        model, opt, start_epoch, history = load_checkpoint(model, opt, checkpoint_path, DEVICE)\n",
        "        for _ in range(start_epoch):\n",
        "            sch.step()\n",
        "\n",
        "    best_loss, best_epoch, bad_epochs, best_state = float('inf'), -1, 0, None\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        running_loss, running_correct, total_samples = 0.0, 0, 0\n",
        "\n",
        "        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for x, y in train_loop:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            if x.ndim == 5:\n",
        "                B, S, C, H, W = x.shape\n",
        "                x = x.view(B * S, C, H, W)\n",
        "                y = y.repeat_interleave(S)\n",
        "\n",
        "            logits = model(x)\n",
        "            loss = crit(logits, y)\n",
        "            loss.backward()\n",
        "\n",
        "            if max_grad_norm > 0:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "            opt.step()\n",
        "\n",
        "            batch_size = y.size(0)\n",
        "            running_loss += loss.item() * batch_size\n",
        "            running_correct += (logits.argmax(1) == y).sum().item()\n",
        "            total_samples += batch_size\n",
        "\n",
        "            train_loop.set_postfix({\n",
        "                'loss': f\"{running_loss/total_samples:.4f}\",\n",
        "                'acc': f\"{100.0*running_correct/total_samples:.2f}%\"\n",
        "            })\n",
        "\n",
        "        train_loss = running_loss / total_samples\n",
        "        train_acc = 100.0 * running_correct / total_samples\n",
        "        cur_lr = opt.param_groups[0][\"lr\"]\n",
        "        sch.step()\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2f}% | LR: {cur_lr:.6f}\")\n",
        "\n",
        "        val_loss, val_acc = None, None\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_running_loss, val_running_correct, val_total_samples = 0.0, 0, 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for x, y in val_loader:\n",
        "                    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(x)\n",
        "                    val_running_loss += crit(logits, y).item() * y.size(0)\n",
        "                    val_running_correct += (logits.argmax(1) == y).sum().item()\n",
        "                    val_total_samples += y.size(0)\n",
        "\n",
        "            val_loss = val_running_loss / val_total_samples\n",
        "            val_acc = 100.0 * val_running_correct / val_total_samples\n",
        "            print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%\\n\")\n",
        "\n",
        "            save_checkpoint(model, opt, epoch, history, filepath=checkpoint_path)\n",
        "\n",
        "            if val_loss < best_loss - min_delta:\n",
        "                best_loss, best_epoch, bad_epochs = val_loss, epoch + 1, 0\n",
        "                best_state = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), os.path.join(checkpoint_dir, \"best_model.pth\"))\n",
        "            else:\n",
        "                bad_epochs += 1\n",
        "                if bad_epochs >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}. Best val loss {best_loss:.4f}\")\n",
        "                    break\n",
        "\n",
        "        history[\"epoch\"].append(epoch + 1)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"lr\"].append(cur_lr)\n",
        "\n",
        "        pd.DataFrame(history).to_csv(csv_path, index=False)\n",
        "\n",
        "    if best_state:\n",
        "        print(f\"Best model at epoch {best_epoch} (val loss {best_loss:.4f})\")\n",
        "        model.load_state_dict(best_state)\n",
        "    else:\n",
        "        print(\"Training completed without validation\")\n",
        "\n",
        "    if plot:\n",
        "        _plot_training_curves(history, show=False, save_dir=save_plots_dir)\n",
        "\n",
        "    return (model, history) if return_history else model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYSYfnYb56-l"
      },
      "outputs": [],
      "source": [
        "#Main\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def main():\n",
        "    set_seed()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    checkpoint_dir = \"checkpoints\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"last.pth\")\n",
        "    csv_path = os.path.join(checkpoint_dir, \"history.csv\")\n",
        "\n",
        "    df_train = pd.read_csv(\"/content/train.csv\")\n",
        "    df_test  = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "    images_np = df_train.drop(\"label\", axis=1).values\n",
        "    labels_np = df_train[\"label\"].values\n",
        "\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        images_np, labels_np, test_size=0.1, random_state=SEED, stratify=labels_np\n",
        "    )\n",
        "\n",
        "    train_dataset = DigitDataset(X_train, y_train, image_shape=IMAGE_SHAPE, transform=train_tf)\n",
        "    val_dataset   = DigitDataset(X_val, y_val, image_shape=IMAGE_SHAPE, transform=val_tf)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = VGG16_net(in_channels=3, n_classes=10).to(device)\n",
        "\n",
        "    model, history = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        checkpoint_dir=checkpoint_dir,\n",
        "        resume=False,          # True nếu muốn load từ checkpoint\n",
        "        plot=True,\n",
        "        return_history=True,\n",
        "        csv_path=csv_path\n",
        "    )\n",
        "\n",
        "    # 7️Lưu model cuối cùng\n",
        "    os.makedirs(\"saved_models\", exist_ok=True)\n",
        "    model_save_path = \"saved_models/vgg16_mnist.pth\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"✅ Model saved to {model_save_path}\")\n",
        "\n",
        "    # 8️⃣ In final loss/accuracy\n",
        "    print(\"Training completed!\")\n",
        "    print(\"Final train loss:\", history[\"train_loss\"][-1])\n",
        "    print(\"Final train acc:\", history[\"train_acc\"][-1])\n",
        "    if history[\"val_loss\"][-1] is not None:\n",
        "        print(\"Final val loss:\", history[\"val_loss\"][-1])\n",
        "        print(\"Final val acc:\", history[\"val_acc\"][-1])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nZiULWOqRdI5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}